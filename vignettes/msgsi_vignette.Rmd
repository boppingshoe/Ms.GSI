---
title: "*Ms.GSI* in the multistage of genetic stock identification madness"
output:
  rmarkdown::html_vignette:
    toc: yes
vignette: >
  %\VignetteIndexEntry{*Ms.GSI* in the multistage of genetic stock identification madness}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(Ms.GSI)
# devtools::load_all()

```

# Overview

This document contains the background information for a integrated multistage genetic stock identification (GSI) model in two parts. The first part the describes how to use package *Ms.GSI* to conduct a GSI analysis. The steps include formatting input data, running the integrated multistage GSI model, and summarizing results. The second part details the technical background of integrated multistage GSI model and its mathematical theory. There is a separate article describe the general background of the integrated multistage framework (*add link to integrated multistage paper*).

# How to use *Ms.GSI*

*Ms.GSI* follows the work flow: format input data, run integrated multistage model, and summarize results/convergence diagnostics.

## Input data

There are few pieces of information needed for input data set:

-   broad-scale baseline
-   regional baseline
-   mixture sample
-   broad-scale population information
-   regional population information

There are pre-loaded example data sets available in *Ms.GSI*. We will look at them one at a time. Note that the example data sets are simulated using existing baseline archived by the Alaska Department of Fish & Game Gene Conservation Lab (GCL). The fabricated data set does not represent the true population proportions in real fisheries. In this example, we made up a scenario similar to the Bering Sea groundfish fisheries where Chinook salmon harvested as bycatch originated from a wide range of geographic areas. Within the bycatch sample, we were interested in the proportion contribution from the lower, middle and upper Yukon River reporting groups (see map below). We used a coast-wide data set for Chinook salmon (Templin et al. 2011; Templin baseline hereafter) as the broad-scale baseline to separate Yukon River fish from other non-Yukon stocks in our data set during the first stage of the analysis. However, genetic markers in the Templin baseline were unable to clearly distinguish between lower Yukon River and other coastal western Alaska populations, so we used a second baseline with additional genetic markers that were specifically designed to differentiate Yukon River Chinook salmon populations (Lee et al. 2021; Yukon River baseline hereafter) as the regional fine-scale baseline for the second stage.

It is important to note that in the original grouping of the Templin baseline, Lower Yukon was a part of Coastal Western Alaska reporting group. We isolated Lower Yukon from the rest of the Coastal Western Alaska, but the accuracy of the proportion estimates would likely diminish because of the breakup of Coastal Western Alaska reporting group. We do not recommend using a genetic baseline beyond its original design. And researchers should be aware of the capability of their genetic baselines before utilizing them in the integrated multistage model. In our example, it would be ideal to keep the Coastal Western Alaska group intact in the broad-scale baseline, and break up the group into Lower Yukon and others using a fine-scale regional baseline. However, at the time of writing, such regional baseline with adequate resolution was still in development.  

![Collection locations and color-coded reporting groups of Chinook salmon represented in the Yukon River example. Shaded area represents the Yukon River region.](small_map_bering_chinook.png){width="100%"}

We assembled a mixture sample containing 150 individuals from collection sites across Yukon River, coastal western Alaskan, Alaska Peninsula, Gulf of Alaska, and Kamchatka Peninsula (Russia). The collections were grouped into five reporting groups: Lower Yukon, Middle Yukon, Upper Yukon, Coastal Western Alaska (Coastal West Alaska), and Others.

### Mixture

First we will take a look at the baseline and mixture samples. *Ms.GSI* accepts the genotype information in two format: 1) GCL format or 2) package *rubias* format. The example data sets are in *rubias* format and naming convention, but procedures for GCL format are the same[^1].

[^1]: Note for the GCL folks: you don't need to transform GCL data sets to *rubias* format. *Ms.GSI* got you covered!

```{r}

print(dplyr::as_tibble(mix))

```

Columns 5 to 358 contain genotype information for loci in **BOTH** broad-scale and regional baselines. You do not have to specify the loci for each baseline (you can if you want to double check, more on that later). *Ms.GSI* matches the loci between mixture and baselines as long as the locus names are consistent.

If you have fish with known-origin, you can specify their identities by adding a column called `known_collection` in the mixture data set. The entry for known-origin should match the collection name in the broad-scale baseline. Fish with unknown-origin should have a `NA` entry.

### Broad-scale baseline

Next, we will take a look at the broad-scale (Templin) baseline example provided in *Ms.GSI*. There are originally 45 loci in the Templin baseline, but we reduced the marker set to 28 loci due to limitation on the data size (and other technical reasons). However, for demonstration purpose, this data set will suffice.

```{r}

print(dplyr::as_tibble(base_templin))

```

### Regional baseline

The regional baseline (Yukon) is in the same format. There are originally 380 loci in the Yukon River Chinook baseline, but we reduced the numbers to 177 in this demonstration.

```{r}

print(dplyr::as_tibble(base_yukon))

```

### Population information

Another piece of information needed is the population details for each baseline. You need to include three columns in the population information table. Column `collection` contains the names for each population in the baseline. Column `reunit` specifies the reporting group each population belongs to. Column `grpvec` specifies the identification number for each reporting group. Below shows the first ten rows of population information for the Templin (broad-scale) baseline.

```{r}

print(dplyr::as_tibble(templin_pops211))

```

If you have hatchery populations in your mixture sample, you can tell *Ms.GSI* either a collection belongs to natural or hatchery-origin by adding an `origin` column in the population information table. In the `origin` column, you identify each collection with `"wild"` or `"hatchery"`. If you don't care to separate natural and hatchery origins, you can lump them in one collection. In this case, you don't need to add an `origin` column. Also, this option to identify hatchery fish is only available in the broad-scale baseline, so don't add an `origin` column in the population table for regional baseline.

Population information table for the Yukon (regional) baseline is in the same format, but not necessarily in the same order.

```{r}

print(dplyr::as_tibble(yukon_pops50))

```

Once you have the data files ready (I recommend saving them as .Rdata files), you can use `prep_msgsi_data()` function to convert them into input data set for model run. You'll also need to identify "groups of interest" in parameter `sub-group`. In this example, groups of interests are Lower Yukon, Middle Yukon and Upper Yukon reporting groups. Their identify numbers are 3, 4, and 5 in the broad-scale baseline. There's an option to save the input data at a designated directory by identifying the location in parameter `file_path`.

`prep_msgsi_data()` function matches the loci between mixture and baselines. But if you want to make sure that you didn't miss any locus in your baselines or mixture, you can manually provide loci names (in string vector) for each baseline by inputting them in `loci1` and `loci2`. In this example we don't manually provide lists of loci because we trust that mixture and baselines all have the correct loci.

```{r}

msgsi_dat <-
  prep_msgsi_data(mixture_data = mix,
  baseline1_data = base_templin, baseline2_data = base_yukon,
  pop1_info = templin_pops211, pop2_info = yukon_pops50, sub_group = 3:5)

```

`prep_msgsi_data()` formats the data files and put them in a list. It took around ten seconds to format the input data in this case. Bigger data sets may take longer. Here are the first few rows/items in the input data list:

```{r}

lapply(msgsi_dat, head)

```

## Genetic stock identification

Once your input data set is ready, you can use `msgsi_mdl()` to run the model. If you are used to running *rubias*, *Ms.GSI* might feel a bit slower. That is because 1) we are running two GSI models in tandem, so it takes twice as long than running a single model, and 2) *Ms.GSI* is written solely in *R*, which is not as computationally efficient as language *C*. So, why not code *Ms.GSI* in *C*? Because we're not technically advanced like the folks who developed *rubias* package (i.e., we don't know how to code in *C++*).

Because of the running time, we recommend running the integrated multistage model in conditional GSI mode (default setting). But there is an option to run the model in fully Bayesian mode if one choose to. If you run the model in fully Bayesian mode, you have the option to include numbers of adaptation run. Some people think that adaptation run encourages convergence in fully Bayesian mode. We have not test that theory but provide the option for those who want to try it.

We demonstrate the model run with one chains of 150 iterations (first 50 as warm-up runs, or burn-ins). We only run one chain in this example so it can pass CMD check while building the vignette document[^#]. In reality, you should **definitely** run multiple chains with more iterations. There are also options to keep the burn-ins and set random seed for reproducible results. We don't show them in this example though (but you can always `?msgsi_mdl`).

[^#]: The package we use to build *Ms.GSI* has an automated checking process. For some reason, automated check would fail if we ran multiple chains in our examples. Probably something to do with the parallel process we included in our function.

```{r}

msgsi_out <- msgsi_mdl(msgsi_dat, nreps = 150, nburn = 50, thin = 1, nchains = 1)

```

## Summarizing results

The output of model contains seven items: `summ_t1`, `trace_t1`, `summ_t2`, `trace_t2`, `summ_comb`, `trace_comb`, and `idens`. Items with "summ" are summary for reporting group proportions and associated convergence diagnostics. If you want to see summaries for stage one and two individually, `summ_t1` and `summ_t2` will show you that. Most people probably want to see the combined summary, `summ_comb`.

```{r}

msgsi_out$summ_comb

```

Most column names are self explanatory, but others might need some additional descriptions. `ci.05` and `ci.95` are the lower and upper bounds of 90% credible interval. `GR` is the Gelman-Rubin statistic (a.k.a. $\hat R$). In this example, Gelman-Rubin statistic is not calculated because we only run one chain. `n_eff` is the effective size, or $N_{eff}$. We will not discuss how to diagnose convergence in this document. Please consult Gelman et al. 2014, Gelman & Rubin 1992, Brooks & Gelman 1998 and other literature on statistical methods.

Items with "trace" are the posterior sample history, or trace history, for either stage one, two, or combined. Trace history is needed for making trace plots. And if you need to combine reporting groups proportions or combine variance, trace histories are what you need. `trace_` items are tibbles with each reporting group as a column. There are two additional columns, `itr` and `chain`, to identify Markov chain Monte Carlo (MCMC) sampling iteration and chain.

```{r}

msgsi_out$trace_comb

```

The last item in the output is the identity assignment history of each individual in the mixture sample. Each column represents an individual in the mixture, and each row records the identity assigned during each iteration in each chain. Individuals are ordered in the same as the input data.

```{r}

msgsi_out$idens

```

*Ms.GSI* has a function for you to make trace plot and examine the mixing of MCMC chains.

```{r, fig.height=12, fig.width=8, out.width="100%", fig.cap="Trace plots for reporting group proportions."}

tr_plot(obj = msgsi_out$trace_comb)

```


$$\newcommand{\mx}[1]{\mathbf{#1}}$$

# Methods (math!)

## Pella-Masuda Model

This integrated multistage GSI model is essentially two Bayesian GSI models stacked on top of each other; hence the name "multistage." The Pella-Masuda model (Pella & Masuda 2001) is the Bayesian GSI model that make up each stage in the integrated multistage model. We will first describe the Pella-Masuda model before discussing the development of a integrated multistage model.

In a group of mixed populations, Pella-Masuda model assigns population identities to each individual based on its genetic make-up (e.g. genotype). Then the model estimates the overall population proportions based on the numbers of individuals assigned to each population. In the fishery context, genetic data of the individuals is called the mixture sample because it consists multi-locus genotype of individual fish collected from a mixed-stock fishery. $\mx x$ denotes the mixture sample. In this document, a bold-font letter represents a *number set*, or a collection of distinct elements. For example, $\mx x$ is a set that contains individual $x$ elements. And $x_{m,l,j}$ is the count of allele $j$ in locus $l$ for individual fish $m$, where $m \in \{1,2,...,M\}$, $l \in \{1,2,...,L\}$, and $j \in \{1,2,...,J_l\}$ depends on locus $l$.

Genetic data of the populations is called the baseline sample because it consists genotype compositions of various baseline populations collected at their spawning locations. Researchers select sampling locations to best represent the populations in an area. $\mx y$ denotes the baseline sample. $y_{k,l,j}$ is the count of allele $j$ in locus $l$ for a sample of size $n_{k,l}$ collected from baseline population $k$, where $k \in \{1,2,...,K\}$.

For both mixture and baseline samples, it is assumed that allele counts in each locus follow a multinomial distribution[^2]. Using another made-up example, in a baseline sample, there are two allele types in locus 1 for population 2. Counts for the two alleles are $y_{1,2,1}, y_{1,2,2}$, and they follow a multinomial distribution with parameters $q_{1,2,1}, q_{1,2,2}$ and size $n_{2,1}$. Note that $q_{1,2,1}, q_{1,2,2}$ are the relative frequencies of the two alleles in locus 1 for population 2. In a Bayesian framework, we need to specify prior distributions for parameters; therefore, we place a Dirichlet[^3] prior distribution on $q_{1,2,1}, q_{1,2,2}$ with hyperparameters[^4] $\beta_{1,1}, \beta_{1,2}$. Usually we set the priors to be equal for all loci. In this example, let $\beta_{1,1} = \beta_{1,2} = 1/2$ based on the number of alleles for locus 1.

[^2]: What is a multinomial distribution? They are the counts of multiple categories based on their corresponding probabilities. For example, we randomly toss 10 Swedish Fish to three kids. Each time we toss a fish, the probability of kid #1 catching it is 0.3, kid #2, 0.3, and kid #3, 0.4. The numbers of fish caught by the three kids are multinomially distributed with probabilities = {0.3, 0.3, 0.4} and a total size 10.

[^3]: What is a Dirichlet distribution? Using the same example for multinomial distribution, the fish catching probabilities of all three kids are Dirichlet-distributed with fish counts of all three kids as parameters. It is worth noting that the parameters do not have to be integers as long as they are \> 0.

[^4]: A hyperparameter is a parameter of a prior distribution.

$\mx q$ represents $q_{1,2,1}$ and $q_{1,2,2}$, together with allele frequencies of other loci and other populations. As you can see, $\mx q$ and $\mx y$ have the same dimension because each relative frequency corresponds to an allele count. In the model, allele frequencies of baseline populations, $\mx q$, determine population proportions. And population proportions determines the identities of individual fish. Individual identities are then tallied and summarized to update baseline allele frequencies. $\mx y$ can be expressed as follows:

$\mx y_k \sim Mult(\mx n_k, \mx q_k)$

Prior distribution for $\mx q$:

$\mx q_k \sim Dirich(\mx \beta)$,

where $\mx \beta = 1/J_l$

For mixture sample, allele counts in each locus of individual fish also follows a multinomial distributions. If a fish came from a certain population, its distribution of allele counts should resemble the allele frequencies of the baseline population which it came from. However, the identity of the individual fish is unknown so it needs to be estimated. Here we let $\mx z_m$ represent the population identify for the $m$^th^ mixture individual. $\mx z_m$ is composed of 0's and an 1 with a length $K$ (e.g. number of baseline populations). $z_{m,k} = 1$ if individual $m$ belongs to population $k$, and $z_{m,k} = 0$ otherwise. In a made-up example, $\mx z_{100} = \{0, 0, 1, 0, 0\}$ means that there are five baseline populations, and individual fish #100 comes from population 3.

We place a multinomial prior on $z_{m,1}, z_{m,2}, ..., z_{m,K}$ with size 1 and probabilities equal to population proportions $p_1, p_2, ..., p_K$. We specify a Dirichlet prior distribution on $p_1, p_2, ..., p_K$ with hyperparameters $\alpha_1, \alpha_2, ..., \alpha_K$, where $\alpha_1 = \alpha_2 = ... = \alpha_K = 1/K$. We usually set $\alpha$ to be equal for all reporting groups, but they can be set based on prior knowledge in population proportions. We express $\mx z$ as follows:

$\mx z_m \sim Mult(\mx 1, \mx p)$

Prior distribution for $\mx p$:

$\mx p \sim Dirich(\mx \alpha)$,

where $\mx \alpha = 1/K$

As mentioned, for mixture sample, allele counts in each locus of individual fish follows a multinomial distributions. The parameters are allele frequencies of the corresponding baseline population with size the numbers of ploidy for each respective locus. Remember that population identity $z_{m,k} = 1$ if individual $m$ belongs to population $k$, and $z_{m,k} = 0$ otherwise. When multiplying population identities, $z_{m,1}, z_{m,2}, ..., z_{m,K}$, and allele frequencies of baseline populations, $\mx q_1, \mx q_2, ..., \mx q_K$, only allele frequencies of baseline population which individual $m$ belong to would remain while the rest goes to zero. $\mx x$ is expressed below. $\mx{ploidy} = ploidy_1, ploidy_2, ..., ploidy_L$ denotes ploidy for each locus.

$\mx x_m \sim Mult(\mx{ploidy}, \mx z_m \cdot \mx q)$

Moran and Anderson (2018) implement a genetic mixture analysis as a *R* package, *rubias*. Their program has been widely used by researchers around the world, including here at the GCL. *rubias* utilizes a model structure called the conditional genetic stock identification model, or the conditional GSI model, that is modified from the Pella-Masuda model. The main difference between the two models is that, in the conditional model, $\mx q$ is integrated out of the distribution of mixture sample, $\mx x_m$. That is, baseline allele frequencies are not being updated in the model. The result of that, $\mx x_m$ takes a form of a compound Dirichlet-multinomial distribution (Johnson at el. 1997):

$\mx x_m \sim CDM(\mx{ploidy}, \mx z_m \cdot \mx v)$,

where $\mx v$ is $\mx \beta + \mx y$. We are not going to attempt proving the theory behind the conditional model in this document (details can be found in Moran & Anderson 2018). But since $\mx q$ has been integrated out of $\mx x_m$, the process for estimating parameters is simpler and more streamlined. We have implemented conditional GSI in each stage of our integrated multistage model.

## Extend to multistage

In the integrated multistage framework, the two Pella-Masuda models are connected because a fish being assigned to a regional group at the second stage is conditional based on whether that fish is assigned to a reporting group in the regional baseline during the first stage. The following describes the conditional relationship between the broad-scale and the regional baselines:

$\mx z^{(regional)}_m | z^{(broad)}_m = \begin{cases} \mx z^{(regional)}_m & \text{if } z^{(broad)}_m = 1\\ \mx 0 & \text{otherwise} \end{cases}$

$\mx z^{(regional)}_m$ are the same as described in the previous section for a single stage scenario. They are vectors of indicators ($0$ or $1$) identifying the regional population that individual $m$ belongs to. $z^{(broad)}_m$ is an indicator as well, and $z^{(broad)}_m = 1$ if individual $m$ belongs to a reporting group within the regional baseline and $0$ otherwise. Combining $z^{(regional)}$ and $z^{(broad)}$, $z^{(regional)}_{m,k} | z^{(broad)}_m = 1$ if individual $m$ belongs to a reporting group within the regional baseline and is assigned to population $k$ in the regional baseline.

Individually, vectors $Z^{(regional)}_m$ have a multinomial distribution, $Z^{(regional)}_m \sim Mult(1, \pi_m)$, where $\pi_m$ are vectors of probabilities for individual $m$ being assigned to each regional reporting group. The prior distribution for $\pi_m$ is a Dirichlet, $Dirich(K^{-1})$, where $K$ is the number of reporting groups in the regional baseline. The posterior distribution for $\pi_m|z^{(regional)}_m$ is also a Dirichlet, $Dirich(\sum_{m=1}^M z^{(regional)}_{m,1} + K^{-1}, \sum_{m=1}^M z^{(regional)}_{m,2} + K^{-1},..., \sum_{m=1}^M z^{(regional)}_{m,K} + K^{-1})$. We will describe in more details on deriving the posteriors in the next section.

In the context of the integrated multistage framework, we only care that, during the first stage, whether an individual is assigned to a reporting group within the regional baseline; therefore, individual identity has a binomial outcome, and $Z^{(broad)}_m \sim Bin(1, \rho_m)$. The prior for $\rho_m$ has a Beta distribution with two scale parameters; one equals to the proportions of broad-scale stocks in the regional baseline, and the other equals to the proportions of broad-scale stocks outside of the regional baseline. The expectation for $\rho_m$ equals to the proportion of broad-scale stocks within the regional baseline. The posterior distribution for $\rho_m|z^{(broad)}_m$ is also a Beta, with the first parameter equals to the sum of individuals assigned to a broad-scale stock in the regional baseline and the proportion of broad-scale stocks in the regional baseline, and the second parameter equals to the sum of individuals assigned to a broad-scale stock outside of the regional baseline and the proportion of broad-scale stocks outside of the regional baseline.

Probability of an individual being assigned to regional population $k$ can be derived using the Bayes rule:

$$\begin{aligned}
P&(Z^{(regional)}_{m,k} | Z^{(broad)}_m) =\\ &\frac{P(Z^{(broad)}_m | Z^{(regional)}_{m,k}) \cdot P(Z^{(regional)}_{m,k})} {\sum_{k=1}^{K} P(Z^{(broad)}_m | Z^{(regional)}_{m,k}) \cdot P(Z^{(regional)}_{m,k})} = \frac{P(Z^{(broad)}_m) \cdot P(Z^{(regional)}_{m,k})}{P(Z^{(broad)}_m)}
\end{aligned}$$

This can be simplified down to just $P(Z^{(regional)}_{m,k})$. Summing up the individual assignment probabilities we get the weighted average of assignment probability to regional population $k$ for all individuals:

$$\begin{aligned}
P&(Z^{(regional)}_k | Z^{(broad)}) =\\ &\sum\limits_{m=1}^{M} \frac{P(Z^{(broad)}_m | Z^{(regional)}_{m,k}) \cdot P(Z^{(regional)}_{m,k})} {\sum_{k=1}^{K} P(Z^{(broad)}_m | Z^{(regional)}_{m,k}) \cdot P(Z^{(regional)}_{m,k})} = \frac{\sum_{m=1}^{M}P(Z^{(broad)}_m) \cdot P(Z^{(regional)}_{m,k})}{\sum_{m=1}^{M} P(Z^{(broad)}_m)}
\end{aligned}$$

During a GSI analysis, two Pella-Masuda models proceed in tandem, each with its own baseline maker suite but the same individuals in the mixture sample.

## Gibbs Sampler: where the fun go round and round

Deriving the values of parameters in each stage of the integrated multistage model requires finding the joint posterior distribution of Pella-Masuda model in each stage, $\mx p, \mx q,\mx z, \mx y, \mx\alpha, \mx\beta$. In this section, we will introduce the concepts and algorithm to sample from this posterior distribution in a single Pella-Masuda model, which then can be extend to a integrated multistage framework.

Gibbs sampler is a type of MCMC methods that sequentially sample parameter values from a Markov chain. With enough sampling, the Markov chain will eventually converge to the desire distribution of interest. The most appealing quality of Gibbs sampler is its reduction of a multivariate problem (such as Pella-Masuda model) to a series of more manageable lower-dimensional problems. A full description of Gibbs sampler and MCMC methods is beyond the scope of this document; however, further information can be found in numerous resources devoting to Bayesian data analysis (see Carlin & Louis 2009; Robert & Casella 2010; Gelman et al. 2014)

To illustrate, suppose we would like to determine the joint posterior distribution of interest, $p(\mx \theta|\mx y)$, where $\mx \theta = \{\theta_1, \theta_2,..., \theta_K\}$. Most likely the multivariate $p(\mx \theta|\mx y)$ would be too complicated to sample from. However, if we can figure out how to break up the joint posterior distribution into individual full conditional distributions, each parameter in $\mx \theta$ can be sampled one by one sequentially using a Gibbs sampler algorithm. The process begins with an arbitrary set of starting values $\theta^{(0)}_2, \theta^{(0)}_3,..., \theta^{(0)}_K$ and proceeds as follows:

For $t = 1,2,...,T$, repeat

1.  Draw $\theta^{(t)}_1$ from $p(\theta_1|\theta^{(t-1)}_2, \theta^{(t-1)}_3,..., \theta^{(t-1)}_k, \mx y)$

2.  Draw $\theta^{(t)}_2$ from $p(\theta_2|\theta^{(t)}_1, \theta^{(t-1)}_3,..., \theta^{(t-1)}_k, \mx y)$

    ⋮

```{=html}
<!-- -->
```
k.  Draw $\theta^{(t)}_k$ from $p(\theta_k|\theta^{(t)}_1, \theta^{(t)}_2,..., \theta^{(t)}_{k-1}, \mx y)$

This would work best if the full conditionals are some known distributions that we can easily sample from (although it's not required). In our case with Pella-Masuda model, we rely on two main concepts, the Bayes theorem and conjugacy, to do the trick. Briefly, for estimating parameters $\mx\theta$ from data $\mx D$, according to Bayes Rule, $p(\mx\theta|\mx D) = \displaystyle \frac{p(\mx D|\mx\theta)p(\mx\theta)}{p(\mx D)}$. $p(\mx\theta|\mx D)$ is the joint posterior distribution for parameters $\mx\theta$, $p(\mx D|\mx\theta)$ is the likelihood of observing the data given the parameters, $p(\mx\theta)$ is the prior distribution of the parameters, and $p(\mx D)$ is the constant marginal distribution of the data. $p(\mx D)$ is often mathematically difficult to obtain; however, because $p(\mx D)$ is a constant number, we can ignore it by reducing the posterior distribution to $p(\mx\theta|\mx D) \propto p(\mx D|\mx\theta)p(\mx\theta)$.

So, how does Bayes Rule help us estimating parameters in the Pella-Masuda model? First, the joint posterior distribution has to be split up into smaller pieces. That is, we separate the joint posterior into likelihood of the data and priors for the parameters:

$p(\mx p, \mx q, \mx z, \mx y, \mx\alpha, \mx\beta)$

$\propto p(\mx x|\mx z, \mx q) p(\mx y|\mx q) \cdot p(\mx p|\mx\alpha) p(\mx q|\mx\beta) p(\mx z|\mx p)$

With some re-arrangements and hand-waving, we arrive at the marginal posterior distributions for $\mx q$ and $\mx p$\$:

$p(\mx x|\mx z, \mx q) p(\mx y|\mx q) \cdot p(\mx p|\mx\alpha) p(\mx q|\mx\beta) p(\mx z|\mx p)$

$= p(\mx x|\mx z, \mx q) p(\mx y|\mx q) p(\mx q|\mx\beta) \cdot p(\mx z|\mx p) p(\mx p|\mx\alpha)$

$\propto p(\mx x,\mx y,\mx z|\mx q) p(\mx q|\mx\beta) \cdot p(\mx z|\mx p) p(\mx p|\mx\alpha)$

$\propto p(\mx q|\mx x,\mx y,\mx z,\mx\beta) \cdot p(\mx p|\mx z,\mx\alpha)$

Next, we take advantage of a mathematical property called *conjugacy* to help us determine the marginal posterior distributions. Based on this property, the posterior distribution follows the same parametric form as the prior distribution when prior is a *conjugate family* for the likelihood. For example, if the likelihood of data is binomial distribution and the prior of parameter is beta distribution, then the posterior is also beta distribution because beta is a conjugate family for binomial. There are many conjugate families, and Dirichlet and multinomial are another example.

Utilizing conjugacy property, we will determine each of the marginal posterior distributions for $\mx q$ and $\mx p$.

### Marginal Posterior p(q\|x, y, z, $\beta$)

We determine that posterior $p(\mx q|\mx x,\mx y,\mx z,\mx\beta)$ is Dirichlet-distributed because Dirichlet prior $p(\mx q|\mx\beta)$ is a conjugate family for the multinomial likelihoods $p(\mx x|\mx z, \mx q)$ and $p(\mx y|\mx q)$. To determine the exact parameterization for the posterior distribution, we need to derive the prior and likelihoods first.

Likelihood $p(\mx x|\mx z, \mx q)$ can be derived in two steps. The first step we conditioned the likelihood on $\mx z$ so that

$p(\mx x|\mx z, \mx q) \propto \displaystyle \prod^{M}_{m=1} \prod^{K}_{k=1} [f(\mx x_m|\mx q_k)]^{z_{m,k}}$,

where $f(\mx x_m|\mx q_k)$ is the relative frequency of multi-locus genotype for individual $m$ in population $k$. In the next step, we derive $f(\mx x_m|\mx q_k)$:

$f(\mx x_m|\mx q_k) \propto \displaystyle \prod^{L}_{l=1} \prod^{J_l}_{j=1} q^{x_{m,l,j}}_{k,l,j}$

Then we combine the two,

$p(\mx x|\mx z, \mx q) \propto \displaystyle \prod^{M}_{m=1} \prod^{K}_{k=1} [f(\mx x_m|\mx q_k)]^{z_{m,k}}$

$\propto \displaystyle \prod^{M}_{m=1} \prod^{K}_{k=1} [\displaystyle \prod^{L}_{l=1} \prod^{J_l}_{j=1} q^{x_{m,l,j} \cdot z_{m,k}}_{k,l,j}]$

$\propto \displaystyle \prod^{K}_{k=1} \prod^{L}_{l=1} \prod^{J_l}_{j=1} q^{\sum^{M}_{m=1} (x_{m,l,j} \cdot z_{m,k})}_{k,l,j}$

Deriving likelihood $p(\mx y|\mx q)$ is more straightforward. It is the product of relative frequency of multi-locus genotype for each population:

$p(\mx y|\mx q) \propto \displaystyle \prod^{K}_{k=1} \prod^{L}_{l=1} \prod^{J_l}_{j=1} q^{y_{k,l,j}}_{k,l,j}$

And $p(q|\mx\beta)$ is Dirichlet prior distribution. Its probability density has a kernel[^6] of $\mx q^{\mx \beta - 1}$. We can express the likelihood as

[^6]: Without involving too much math, *kernel* here refers to what is left over after factoring out the constant from a probability density function (PDF). For example, The PDF of Dirichlet distribution is $\frac{1}{B(\mx \alpha)} \displaystyle \prod^K_{i=1} x^{\alpha_i - 1}_i$. Beta function $B(\mx \alpha)$ can be factored out, and $\displaystyle \prod^K_{i=1} x^{\alpha_i - 1}_i$ is the kernel.

$p(\mx q|\mx\beta) \propto \displaystyle \prod^{K}_{k=1} \prod^{L}_{l=1} \prod^{J_l}_{j=1} q^{\beta_{l,j} - 1}_{k,l,j}$.

Put all the likelihoods together,

$p(\mx q|\mx x,\mx y,\mx z,\mx\beta) \propto p(\mx x|\mx z, \mx q) p(\mx y|\mx q) p(\mx q|\mx\beta)$

$\propto \displaystyle \prod^{K}_{k=1} \prod^{L}_{l=1} \prod^{J_l}_{j=1} q^{\sum^{M}_{m=1} (x_{m,l,j} \cdot z_{m,k})}_{k,l,j} \cdot \displaystyle \prod^{K}_{k=1} \prod^{L}_{l=1} \prod^{J_l}_{j=1} q^{y_{k,l,j}}_{k,l,j} \cdot \displaystyle \prod^{K}_{k=1} \prod^{L}_{l=1} \prod^{J_l}_{j=1} q^{\beta_{l,j} - 1}_{k,l,j}$

$= \displaystyle \prod^{K}_{k=1} \prod^{L}_{l=1} \prod^{J_l}_{j=1} q^{\sum^{M}_{m=1} (x_{m,l,j} \cdot z_{m,k}) + y_{k,l,j} + \beta_{l,j} - 1}_{k,l,j}$

It is *elementary* for anybody to recognize that $\displaystyle \prod^{K}_{k=1} \prod^{L}_{l=1} \prod^{J_l}_{j=1} q^{\sum^{M}_{m=1} (x_{m,l,j} \cdot z_{m,k}) + y_{k,l,j} + \beta_{l,j} - 1}_{k,l,j}$ is the kernel for Dirichlet distribution. Hence,

$\mx q_{k,l}|\mx x,\mx y,\mx z,\mx\beta \sim Dirich(\displaystyle \sum^{M}_{m=1} x_{m,l,j} z_{m,k} + y_{k,l,j} + \beta_{l,j})$

### Marginal Posterior p(p\|z, $\alpha$)

Using the same logic as previously, $p(\mx p|\mx z,\mx\alpha)$ is also Dirichlet-distributed due to a Dirichlet prior $p(\mx p|\mx\alpha)$ and a multinomial likelihood $p(\mx z|\mx p)$.

$p(\mx p|\mx z,\mx\alpha) \propto p(\mx z|\mx p) p(\mx p|\mx\alpha)$

$\propto \displaystyle \prod^{M}_{m=1} \prod^{K}_{k=1} p^{z_{m,k}}_k \cdot \prod^{K}_{k=1} p^{\alpha_k - 1}_k$

$\propto \displaystyle \prod^{K}_{k=1} p^{\sum^M_{m=1}z_{m,k} + \alpha_k - 1}_k$

Once again, we recognize it as the kernel for Dirichlet distribution:

$\mx p|\mx z,\mx\alpha \sim Dirich(\displaystyle \sum^M_{m=1}z_{m,k} + \alpha_k)$

### Algorithm

There is one more distribution to figure out before we can start our Gibbs sampler routine (and you thought we're all set, lol). We would need to know how to sample $\mx z_m|\mx p, \mx q, \mx x_m$, the population identity for individual fish $m$ (in components 1 and 2) given the population proportions and genotype. If the probability of fish $m$ belong to population $k$ is $p_k$, and the likelihood of observing relative frequency of genotype for fish $m$ in population $k$ is $f(\mx x_m|\mx q_k)$, then the probability of fish $m$ belong to population $k$ given the population proportions and genotype is $\displaystyle \frac{p_k \cdot f(\mx x_m|\mx q_k)}{\sum^K_{k'=1}p_{k'} \cdot f(\mx x_m|\mx q_{k'})}$. The denominator should sum to one, so we only need to calculate the numerator.

$\mx z_m|\mx p, \mx q, \mx x_m$ has the following distribution:

$\mx z_m|\mx p, \mx q, \mx x_m \sim Mult(1, \mx{p'}_m)$,

where $p'_{m,k} = p_k \cdot f(\mx x_m|\mx q_k)$. We draw the initial values for $\mx q_k$ based on its prior distribution.

Once we figured out all the pieces in the Gibbs sampler, we may begin the process with starting values for $\mx p^{(0)}$ and $\mx q^{(0)}$. We proceed as follows:

For $t = 1,2,...,T$, repeat

1.  Determine the population identity of mixture individuals, $\mx z^{(t)}_m|\mx p^{(t-1)}, \mx q^{(t-1)}, \mx x_m \sim Mult(1, \mx{p'}_m)$.

2.  Draw marginal posterior distributions for $\mx q^{(t)}$ and $\mx p^{(t)}$ from $p(\mx q|\mx x,\mx y,\mx z^{(t)},\mx\beta)$ and $p(\mx p|\mx z^{(t)},\mx\alpha)$ respectively.

$T$ should be large enough to ensure the sampler chain converges to desire distribution of interest. Usually it takes thousands of iterations.

To extend the algorithm to multistage:

1.  Determine the population identities of mixture individuals at the first stage, $\mx z^{(broad)(t)}_m|\mx p^{(broad)(t-1)}, \mx q^{(broad)(t-1)}, \mx x_m \sim Mult(1, \mx{p'}^{(broad)}_m)$.

2.  Determine the population identities of mixture individuals at the second stage, $\mx z^{(regional)(t)}_m|z^{(broad)(t)}_m, \mx p^{(regional)(t-1)}, \mx q^{(regional)(t-1)}, \mx x_m \sim Mult(1, \mx{p'}^{(regional)}_m)$.

3.  Draw marginal posterior distributions for $\mx q^{(broad)(t)}$, $\mx p^{(broad)(t)}$, $\mx q^{(regional)(t)}$ and $\mx p^{(regional)(t)}$ from $p(\mx q^{(broad)}|\mx x,\mx y^{(broad)},\mx z^{(broad)(t)},\mx\beta^{(broad)})$, $p(\mx p^{(broad)}|\mx z^{(broad)(t)},\mx\alpha^{(broad)})$, $p(\mx q^{(regional)}|\mx x,\mx y^{(regional)},\mx z^{(regional)(t)},\mx\beta^{(regional)})$ and $p(\mx p^{(regional)}|\mx z^{(regional)(t)},\mx\alpha^{(regional)})$ respectively.

Steps 1 through 3 are repeated until simulations converge to a desire distribution.

Implementing the conditional GSI model only requires a slight modification from the above algorithm. At the first stage, $f(\mx x_m|\mx q^{(broad)}_k)$ will only need to be derived once in the beginning of the process, and $\mx q^{(broad)}$ is no longer updated in step 3. At the second stage, $\mx x_m$ changes depending on the outcome of $z^{(broad)}_m$; therefore, $f(\mx x_m|\mx q^{(regional)}_k)$ is recalculated. However, $\mx q^{(regional)}$ does not need to be updated. Everything else in the algorithm stays the same.

# References

Brooks, S. P., and A. Gelman. 1998. General methods for monitoring convergence of iterative simulations. *Journal of Computational and Graphical Statistics*. 7:434--455.

Carlin, B. and T. Louis. 2009. *Bayesian Methods for Data Analysis, 3rd Edition*. CRC Press. New York.

Gelman, A., and D. B. Rubin. 1992. Inference from iterative simulation using multiple sequences. *Statistical Science*. 7:457--472.

Gelman, A., J. Carlin, H. Stern, D. Dunson, A. Vehtari and D. Rubin. 2014. *Bayesian Data Analysis, 3rd Edition*. CRC Press. New York.

Johnson, N.L., Kotz, S., and Balakrishnan, N. 1997. Discrete multivariate distributions. Wiley & Sons, New York.

Lee. E., T. Dann, and H. Hoyt. 2021. Yukon River Chinook Genetic Baseline Improvements. Yukon River Panel Restoration and Enhancement Fund Final Report, URE-163-19N.

Moran, B.M. and E.C. Anderson. 2018. Bayesian inference from the conditional genetic stock identification model. *Canadian Journal of Fisheries and Aquatic Sciences*. 76(4):551-560. <https://doi.org/10.1139/cjfas-2018-0016>

Pella, J. and M. Masuda. 2001. Bayesian methods for analysis of stock mixtures from genetic characters. *Fish. Bull.* 99:151--167.

Robert, C. and G. Casella. 2010. *Introducing Monte Carlo Methods with R*. Springer. New York.

Templin, W. D., J. E. Seeb, J. R. Jasper, A. W. Barclay, L. W. Seeb. 2011. Genetic differentiation of Alaska Chinook salmon: the missing link for migratory studies. *Mol Ecol Resour*. 11(Suppl 1):226-246. <doi:10.1111/j.1755-0998.2010.02968.x>.
